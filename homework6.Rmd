---
title: "Homework6"
author: "Xinyao Wu"
date: "2018/11/16"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(broom)
library(modelr)
library(mgcv)
library(leaps)
```

```{r}
knitr::opts_chunk$set(
  fig.width = 12,
  fig.asp = .6,
  out.width = "100%"
)
```

##problem1
__Washington Post__

#####(1) Tidy Data

_disposition: new binary variable_

_delete records:Dallas, TX; Phoenix, AZ; and Kansas City, MO_

_exhibit head 5 lines of dataset_

```{r}
Original_data = read.csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") 
wp_df = 
  Original_data %>% 
  janitor::clean_names() %>% 
  #Create a city_state variable and a binary variable indicating whether the homicide is solved.
  mutate(
    city_state = str_c(city,state, sep = ", "), 
    resolved = as.numeric(disposition == "Closed by arrest")
  ) %>% 
  #Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO
  filter(
    city_state != "Dallas, TX" & city_state !="Phoenix, AZ" & city_state !="Kansas City, MO" & city_state !="Tulsa, AL"
  ) %>% 
  mutate(
    victim_race = ifelse(victim_race =="White","white","non_white"),
    victim_race = fct_relevel(victim_race, "white"),
    victim_age = as.numeric(victim_age)
  )
wp_df %>% head(5) 

```


#####(2)Fit a logistic regression for "Baltimore, MD"

```{r collapse=TRUE}
#For the city of Baltimore, MD
baltimore = wp_df %>% 
  filter(city_state =="Baltimore, MD") 
#fit a logistic regression
fit_baltimore = glm(resolved ~ victim_age + victim_sex + victim_race , family = binomial(), data = baltimore)

broom::tidy(fit_baltimore)
#the estimate of the adjusted odds ratio for solving homicides comparing non-white victims to white victims
coef(fit_baltimore)["victim_racenon_white"] %>% exp()
#the confidence interval of the adjusted odds ratio for solving homicides comparing non-white victims to white victims
confint(fit_baltimore,"victim_racenon_white")%>% exp()
```

The estimate of the adjusted odds ratio for solving homicides comparing non-white victims to white victims is 0.453, and the 95% CI is (0.321,0.636).

#####(3)Glm for each of the cities and extract the adjusted odds ratio (and CI) 

```{r warning=FALSE}
 city_glm =
  wp_df %>% 
  select(resolved, city_state, victim_race,victim_age,victim_sex) %>% 
  group_by(city_state) %>% 
  nest() %>% 
  mutate(
    model = map(data,~glm(resolved ~ victim_age + victim_sex + victim_race ,family = binomial(),data = .)),
    model = map(model, ~broom::tidy(.,conf.int = TRUE))
)  %>% 
   select(-data) %>% 
  unnest() %>% 
  filter(term == "victim_racenon_white")  %>% 
  mutate(OR = exp(estimate),
         CI.low = exp(conf.low),
         CI.high = exp(conf.high)) %>% 
  select(city_state,OR,CI.low ,CI.high ) 

 knitr::kable(city_glm)
```

#####(5)Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.

```{r}
city_glm %>% 
  mutate(
    city_state = reorder(city_state,desc(OR))
  ) %>% 
ggplot(aes(x = city_state, y = OR))+ geom_line()+
  geom_point()+
  geom_errorbar(aes(x= city_state,ymin = CI.low, ymax = CI.high))+
  labs(
    x = "City, State",
    y = "Adjusted Odd ratio ",
    title = "odd ratio of homocides solving comparing non-white to white victims in United States"
  )+
  theme(
    axis.text.x = element_text(angle = 45 , hjust = 1)
  )
```

##Comment:

Most cities who have a higher OR are tending to have a wider OR confidence interval.`Tampa,Fl`,`Durham,NC`,`Birmingham,AL` these three cities have a OR higher than 1, which indicates in these cities, homicides with non-white victims are more likely to be solved. However, since they all have a wide 95% confidence interval, which include point 1, there still exists some probability that homicides with non-white victims are equal or less likely to be solved.

##problem2

####(1)Load and clean the data for regression analysis
```{r}
orignal_bw = read.csv("./data/birthweight.csv")
bw_df = orignal_bw %>% 
  janitor::clean_names() %>% 
  #convert numeric to factor where appropriate
  mutate(
    babysex = as.factor(ifelse(babysex==1,"male","female")),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace)
  )
#check for missing data
table(is.na(bw_df))
```

####(2)Propose a regression model for birthweight. 

This model may be based on a hypothesized structure for the factors that underly birthweight, on a data-driven model-building process, or a combination of the two. __Describe your modeling process__ and show a plot of model residuals against fitted values – use  add_predictions and add_residuals in making this plot.

__1)check the distribution of `bwt`__
```{r}
bw_df %>% 
#the distribution of baby’s birth weight
ggplot(aes(x = bwt))+geom_histogram()
#likely normal
```
__2)choose predictors__

Although there are variables,such as `blength`,`bhead`,`baby sex` and so on, that cannot be seen as logic predictors for birthweight since they are all outcomes as birthweight, to more specificly know the p-value of all the variables, we make a total linear regression first.
```{r}
choose_predi = lm(bwt~.,data = bw_df)
broom::tidy(choose_predi) %>% 
  select(term,p.value) %>% 
 arrange(p.value) 
```

The three lowest p-value owner are `bhead`,`blength`,`delwt`.However, obviously they are not factors that can underly birthweight except `delwt`. Sometimes we keep such variables to adjust the coefficient value for other variables so we are not sure whether to delet such variables from modeling or not. Thus, I will make a new model mainly based on data-driven model-building process.
 
```{r}
lm(bwt ~ ., data = bw_df) %>% 
  step(direction='both')
```

To aviod overfit, we only choose 6 factors as our model`s predictors.

```{r}
fit_test1 = lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = bw_df)
broom::tidy(fit_test1) %>% arrange(p.value)
```
 We will choose variables based on the rank of p-value above, that is: `bhead`,`blength`,`mrace`,`delwt`,`smoken`,`gaweeks`.
 Then we will see how fittable of this model:
```{r}
fit_test2 = lm(bwt~bhead+blength+mrace+delwt+smoken+gaweeks,data = bw_df)
broom::glance(fit_test2)
```
 
The R_squared is 0.713, which seems it fits well.
 

__3)Regression model and Plot__

Sincebirthweight is appromately normal distribution, we directly use linear regression.

```{r}
#make a linear regression
  fit_self = lm(bwt~bhead+blength+mrace+delwt+smoken+gaweeks,data = bw_df)
  summary(fit_self)
   reg = bw_df %>% 
     select(bwt,bhead,blength,mrace,delwt, smoken, gaweeks) 
 pred_resid = modelr::add_predictions(reg, fit_self) 
 pred_resid$resid = modelr::add_residuals(reg, fit_self) %>% pull(resid)
 
  ggplot(pred_resid,aes(x = pred, y = resid))+
  geom_point(alpha = 0.3 )+
     geom_smooth(se = FALSE)
    labs(
      title = "predictions vs residuals of the specific linear regression model",
      y = "residual",
      x = "prediction"
    )
```

##comments
The residuals do not have a desired horizontally symmetric distributed pattern. Espectially when prediction are (0,1000), most residuals seems to have  positive value. Meanwhile, 5 points` residual are beyond 1000 as well as 2 are beyond -1000. However, Most residuals lied between (-1000,+1000). We cannot expect all of the points to have a perfect horizontally symmetric distribution in reality, so this model fits well.

##Compare my model to two others:

__cv set__

```{r}
cv_df =
  crossv_mc(bw_df, 100) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))
```

__plot rmse distribution__
```{r}
cv_df = cv_df %>% 
  mutate(
    non_nest_mod = map(train,~lm(bwt~blength + gaweeks,data = .x)),
    nest_mod = map(train,~lm(bwt~bhead * blength * babysex,data = .x)),
    self_mod = map(train,~lm(bwt~bhead+blength+mrace+delwt+smoken+gaweeks,data = bw_df))
  ) %>% 
  mutate(
    rmse_non_nest   = map2_dbl(non_nest_mod, test, ~rmse(model = .x, data = .y)),
    rmse_nest   = map2_dbl(nest_mod, test, ~rmse(model = .x, data = .y)),
    rmse_self   = map2_dbl(self_mod, test, ~rmse(model = .x, data = .y))
  )

cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = "model",value = rmse) %>%
  mutate(
    model = str_replace(model,"rmse_",""),
    model = fct_inorder(model)
         ) %>% 
  ggplot(aes(x = model, y = rmse))+geom_violin()+
  labs(
    title = "Rmse distribution in three different model cross-validations"
  )

```

My model which set `delwt`(mother’s weight at delivery,pounds),`bhead`(baby’s head circumference at birth,centimeters),`blength`(baby’s length at birth,centimeteres) ,`mrace`( mother’s race (1= White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other)),`smoken`(average number of cigarettes smoked per day during pregnancy),`gaweeks`(gestational age in weeks) as predictors has a subtle lower rmse than the other two models. 

```{r}
broom::tidy(fit_self) %>% knitr::kable()
```

Meanwhile, there are the coefficiences of my model, from which we can see that :(1)compared to white woman, black and Puerto Rican women tend to diliver babies with less wight on the significance of 0.01.(2)when mother’s weight at delivery increases by 1 pounds, we may expect 2.28 grams increasing on babies weight.(3)when mother’s average number of cigarettes smoked per day during pregnancy increases by 1 , we may expect 4.84 grams losting on babies weight.(4)when mother’s gestational age increases by 1 week,we may expect 12.34 grams increasing on babies weight.













